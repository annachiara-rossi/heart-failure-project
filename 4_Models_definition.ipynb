{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4734d2bc",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b783ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/annachiararossi/Documents/Healthcare data/heart-failure-project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603de935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.random.seed(130298) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05a19b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer \n",
    "from fancyimpute import IterativeImputer, KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06ae280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import auc, f1_score, recall_score, precision_score, roc_curve, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b202d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2117f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(Y, final_pred):\n",
    "    fpr, tpr, _ = roc_curve(Y,final_pred)\n",
    "    AUC = auc(fpr, tpr)\n",
    "    f1 = f1_score(Y,final_pred)\n",
    "    rec = recall_score(Y,final_pred)\n",
    "    prec = precision_score(Y,final_pred)\n",
    "    print(f'AUC: {AUC}, F1: {f1}, Recall: {rec}, Precision: {prec}')\n",
    "    return [AUC, f1, rec, prec]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eee318",
   "metadata": {},
   "source": [
    "## Datasets definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2f1346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_name = 're.admission.within.6.months'\n",
    "\n",
    "# take the training set\n",
    "X_train = pd.read_csv('train_data_drugs.csv')\n",
    "X_train.set_index('inpatient.number', inplace = True)\n",
    "\n",
    "# separate the outcome\n",
    "Y_train = X_train[outcome_name].copy()\n",
    "X_train.drop(columns = outcome_name, inplace = True)\n",
    "\n",
    "# take the test set\n",
    "X_test = pd.read_csv('test_data_drugs.csv')\n",
    "X_test.set_index('inpatient.number', inplace = True)\n",
    "\n",
    "# separate the outcome\n",
    "Y_test = X_test[outcome_name].copy()\n",
    "X_test.drop(columns = outcome_name, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa8a62a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of traing set: (1573, 82) and of test set: (397, 82)\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of traing set: {X_train.shape} and of test set: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de33da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformations\n",
    "var_to_log = ['creatinine.enzymatic.method', 'urea', 'glomerular.filtration.rate', \n",
    "              'cystatin', 'lymphocyte.count', 'neutrophil.count',\n",
    "              'activated.partial.thromboplastin.time', 'prothrombin.time.ratio',\n",
    "              'glutamyltranspeptidase','indirect.bilirubin','alkaline.phosphatase',\n",
    "              'globulin','direct.bilirubin', 'low.density.lipoprotein.cholesterol', \n",
    "              'triglyceride']\n",
    "X_train[var_to_log] = np.log(X_train[var_to_log])\n",
    "X_test[var_to_log] = np.log(X_test[var_to_log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c41254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the lists of variables by type\n",
    "\n",
    "cat_columns = ['DestinationDischarge','admission.ward','admission.way','discharge.department',\n",
    "                       'type.of.heart.failure', 'NYHA.cardiac.function.classification', 'Killip.grade',\n",
    "                       'consciousness', 'ageCat']\n",
    "\n",
    "ordinal_columns = ['CCI.score', 'eye.opening','verbal.response', 'movement', 'GCS']\n",
    "\n",
    "not_continuous = cat_columns.copy()\n",
    "\n",
    "binary_columns = ['gender', 'diabetes', 'moderate.to.severe.chronic.kidney.disease',\n",
    "                  'return.to.emergency.department.within.6.months', 'diuretics',\n",
    "                  'hypertension', 'heart_failure', 'angina_etal', 'cholesterol']\n",
    "\n",
    "not_continuous.extend(binary_columns)\n",
    "not_continuous.extend(ordinal_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73b623b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_columns = [col_name for col_name in X_train.columns if col_name not in not_continuous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13ece53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each category we add its specification, needed to retreive columns after OneHotEcoding\n",
    "#for cat in cat_columns:\n",
    "#    X_train[cat] = cat + '_' + X_train[cat]\n",
    "#    X_test[cat] = cat + '_' + X_test[cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8dd8c7",
   "metadata": {},
   "source": [
    "## Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a59a64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def log_transf_stand(x):\n",
    "#    \"Transform to log and standardize\"\n",
    "#    x = np.log(x)\n",
    "#    return (x - x.mean())/x.std()\n",
    "\n",
    "#log_transformer = FunctionTransformer(log_transf_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7112a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_pipeline = ColumnTransformer([\n",
    "#        (\"log\", log_transformer, var_to_log),\n",
    "#        (\"num\", StandardScaler(), list(set(cont_columns) - set(var_to_log))),     \n",
    "#        (\"cat\", OneHotEncoder(), cat_columns)],\n",
    "#       remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e8ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = ColumnTransformer([\n",
    "                (\"num\", StandardScaler(),cont_columns),\n",
    "                (\"cat\", OneHotEncoder(), cat_columns)],\n",
    "                remainder = 'passthrough') # do not modify columns not listed\n",
    "\n",
    "X_train_prepared = full_pipeline.fit_transform(X_train)\n",
    "X_test_prepared = full_pipeline.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87460e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(remainder='passthrough',\n",
       "                  transformers=[('num', StandardScaler(),\n",
       "                                 ['body.temperature', 'pulse', 'respiration',\n",
       "                                  'systolic.blood.pressure',\n",
       "                                  'diastolic.blood.pressure', 'weight', 'BMI',\n",
       "                                  'fio2',\n",
       "                                  'left.ventricular.end.diastolic.diameter.LV',\n",
       "                                  'creatinine.enzymatic.method', 'urea',\n",
       "                                  'uric.acid', 'glomerular.filtration.rate',\n",
       "                                  'cystatin', 'monocyte.count',\n",
       "                                  'r...\n",
       "                                  'hemoglobin', 'platelet',\n",
       "                                  'platelet.distribution.width',\n",
       "                                  'neutrophil.count', 'D.dimer',\n",
       "                                  'activated.partial.thromboplastin.time',\n",
       "                                  'thrombin.time', ...]),\n",
       "                                ('cat', OneHotEncoder(),\n",
       "                                 ['DestinationDischarge', 'admission.ward',\n",
       "                                  'admission.way', 'discharge.department',\n",
       "                                  'type.of.heart.failure',\n",
       "                                  'NYHA.cardiac.function.classification',\n",
       "                                  'Killip.grade', 'consciousness', 'ageCat'])])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caf27fdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ColumnTransformer' object has no attribute 'get_feature_names_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-94567cabcf5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcolumn_prepared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ColumnTransformer' object has no attribute 'get_feature_names_out'"
     ]
    }
   ],
   "source": [
    "column_prepared = full_pipeline.get_feature_names_out(input_features=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7126f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prepared=pd.DataFrame(X_train_prepared, columns=column_prepared, index=X_train.index)\n",
    "X_train_prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcae78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prepared=pd.DataFrame(X_test_prepared, columns=column_prepared, index=X_test.index)\n",
    "X_test_prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2418774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prepared.drop(columns = \"cat__DestinationDischarge_nan\",inplace=True)\n",
    "X_test_prepared.drop(columns = \"cat__DestinationDischarge_nan\",inplace=True)\n",
    "column_prepared.remove(\"cat__DestinationDischarge_nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e6762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Size of traing set: {X_train_prepared.shape} and of test set: {X_test_prepared.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b40b08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# imputation cannot be done inside the Pipeline because the method 'transform' is missing\n",
    "\n",
    "knn_imputer = KNN()\n",
    "X_train_prepared = knn_imputer.fit_transform(X_train_prepared)\n",
    "X_test_prepared = knn_imputer.fit_transform(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c3d47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'Size of traing set: {X_train_prepared.shape} and of test set: {X_test_prepared.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8714d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for idx,elem in enumerate(features_names):\n",
    "#    if elem == 'nan':\n",
    "#        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_prepared = np.delete(X_train_prepared, idx, axis = 1)\n",
    "#X_test_prepared = np.delete(X_test_prepared, idx, axis = 1)\n",
    "\n",
    "# ADJUST THIS!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5be909",
   "metadata": {},
   "source": [
    "## Logistic Regression with Elasticnet penalty\n",
    "Since we have many columns, of which we doubt some might still be collinear, we use an Elastic net penalty, which takes into account both the L2 norm and the L1 norm, which induces sparsity.\n",
    "\n",
    "Moreover, we add the “balanced” mode, which uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc828ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_clf = LogisticRegression(penalty = 'elasticnet', solver = 'saga', \n",
    "                                 class_weight = 'balanced', l1_ratio = 0.5, \n",
    "                                 max_iter = 700, random_state=42)\n",
    "# this method takes many iterations to converge\n",
    "log_reg_clf.fit(X_train_prepared, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e04ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = log_reg_clf.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(Y_test, pred)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec389c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "model = SelectFromModel(log_reg_clf, prefit=True)\n",
    "X_new = model.transform(X_train_prepared)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0726b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features=model.get_feature_names_out(input_features=column_prepared)\n",
    "significant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61665ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO GET THE NAMES OF THE IMPORTANT FEATURES, WE HAVE TO PASS THE MATRIX WITH THE COLUMN NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFTER THIS, FIT THE RANDOM FOREST ONLY WITH THE GOOD FEATURES!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef77a2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#log_reg_clf = LogisticRegression(penalty = 'elasticnet', solver = 'saga', class_weight = 'balanced', max_iter = 1000, random_state=42)\n",
    "\n",
    "# C and l1_ratio can be tuned\n",
    "#'C': np.linspace(0.1,5,100),\n",
    "#param_grid = {'l1_ratio': np.linspace(0,1,101)}\n",
    "#grid_search = GridSearchCV(log_reg_clf, param_grid, cv=10,\n",
    "#                           scoring='neg_mean_squared_error',\n",
    "#                           return_train_score=True)\n",
    "#grid_search.fit(X_train_prepared, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb83991",
   "metadata": {},
   "source": [
    "## Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5bd4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [10, 30, 50, 100], \n",
    "     'max_features': list(np.arange(10,X_train_prepared.shape[1],10)) + [X_train_prepared.shape[1]]} \n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    \n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_clf, param_grid, cv=10,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(X_train_prepared, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810809d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c3cd3",
   "metadata": {},
   "source": [
    "### Fetaure importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e054c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = final_model.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3573e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_list_one_hot = full_pipeline.named_transformers_[\"cat\"].categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_features = []\n",
    "#for elem in cat_list_one_hot:\n",
    "#    cat_features.extend(elem)\n",
    "#cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(feature_importances,features_names), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a2be61",
   "metadata": {},
   "source": [
    "### Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2fa6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(Y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
