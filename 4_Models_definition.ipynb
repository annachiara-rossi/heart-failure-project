{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "np.random.seed(130298) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as Pipeline_imb\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer \n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2, RFECV, SelectFpr, SelectFdr\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import auc, f1_score, recall_score, precision_score, roc_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(Y, final_pred):\n",
    "    fpr, tpr, _ = roc_curve(Y,final_pred)\n",
    "    AUC = auc(fpr, tpr)\n",
    "    f1 = f1_score(Y,final_pred)\n",
    "    print(f'AUC: {np.round(AUC,3)}, F1: {np.round(f1,3)}')\n",
    "    return [AUC, f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: (1567, 81) and of test set: (397, 81)\n"
     ]
    }
   ],
   "source": [
    "outcome_name = 're.admission.within.6.months'\n",
    "\n",
    "# take the training set\n",
    "X_train = pd.read_csv('train_data_drugs.csv')\n",
    "X_train.set_index('inpatient.number', inplace = True)\n",
    "\n",
    "# separate the outcome\n",
    "Y_train = X_train[outcome_name].copy()\n",
    "X_train.drop(columns = outcome_name, inplace = True)\n",
    "\n",
    "# take the test set\n",
    "X_test = pd.read_csv('test_data_drugs.csv')\n",
    "X_test.set_index('inpatient.number', inplace = True)\n",
    "\n",
    "# separate the outcome\n",
    "Y_test = X_test[outcome_name].copy()\n",
    "X_test.drop(columns = outcome_name, inplace = True)\n",
    "print(f'Size of training set: {X_train.shape} and of test set: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to transform some variables (as identified in the file 2_Data_cleaning) with a skewed distribution using the logarithm: a distribution with less heavy tails can help the models to learn better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformations\n",
    "var_to_log = ['creatinine.enzymatic.method', 'urea', 'glomerular.filtration.rate', \n",
    "              'cystatin', 'lymphocyte.count', 'neutrophil.count',\n",
    "              'activated.partial.thromboplastin.time', 'prothrombin.time.ratio',\n",
    "              'glutamyltranspeptidase','indirect.bilirubin','alkaline.phosphatase',\n",
    "              'globulin','direct.bilirubin', 'low.density.lipoprotein.cholesterol', \n",
    "              'triglyceride']\n",
    "X_train[var_to_log] = np.log(X_train[var_to_log])\n",
    "X_test[var_to_log] = np.log(X_test[var_to_log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the lists of variables by type\n",
    "cat_columns = ['DestinationDischarge','admission.ward','admission.way','discharge.department',\n",
    "                       'type.of.heart.failure', 'NYHA.cardiac.function.classification', 'Killip.grade',\n",
    "                       'consciousness', 'ageCat']\n",
    "\n",
    "ordinal_columns = ['CCI.score', 'eye.opening','verbal.response', 'movement', 'GCS']\n",
    "\n",
    "binary_columns = ['gender', 'diabetes', 'moderate.to.severe.chronic.kidney.disease',\n",
    "                  'diuretics', 'hypertension', 'heart_failure', 'angina_etal', 'cholesterol']\n",
    "\n",
    "not_continuous = cat_columns.copy()\n",
    "not_continuous.extend(binary_columns)\n",
    "not_continuous.extend(ordinal_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_columns = [col_name for col_name in X_train.columns if col_name not in not_continuous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS A CATEGORICAL FEATURE WHICH WILL BE ONE-HOT-ENCODED BUT HAS SOME NaN. \n",
    "# To avoid problems, we impute it before with the mode\n",
    "\n",
    "X_train['DestinationDischarge'].fillna(X_train['DestinationDischarge'].mode().values[0], inplace=True)\n",
    "X_test['DestinationDischarge'].fillna(X_train['DestinationDischarge'].mode().values[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the processing of our data through a Pipeline: first we standardized the continuous variables, and we encoded categorical variables using One Hot Encoding. Then, since in the dataset there are some missing values, we implement a KNNImputer which imputes each one of them using the average of the 5 nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oneHot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessing_pipeline = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), cont_columns),\n",
    "    (\"cat\", oneHot_encoder, cat_columns)],  \n",
    "    remainder = 'passthrough',               \n",
    "    verbose_feature_names_out = False)       \n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing_pipeline),\n",
    "    ('imputer', KNNImputer())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate all methods by cross-validating the performance on the training set. The same CV scheme is use for all models, and consequently results are comparable.\n",
    "We choose the $F1$ score since it is an average between Recall and Precision, taking into account both mistakes on the positive and on the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the algorithms which also needed tuning, we implement a nested cross-validation strategy: it allows to find parameters trough a Grid-Search for every cross validation fold and at the same time to get an estimate of the chosen score. This ensures the significance of the estimates and avoids optimistic measures of performance, which would then be overturned by testing the final model on the hold-out set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since collinearity among features might still be present, we considered an Elastic-Net penalty, which takes into account both the L2 norm and the L1 norm, which induces sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the saga solver, since the ElasticNet regularization is only supported by that and we set the balance mode which uses the values of Y_train to automatically adjust weights inversely proportional to class frequencies in the input data. We increased the number of iterations to guarantee the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnet_model = LogisticRegression(penalty = 'elasticnet', solver = 'saga', \n",
    "                                 class_weight = 'balanced', max_iter = 5000, random_state=42)\n",
    "\n",
    "#Pipeline: preprocessing + Logistic regression model with elasticnet penalization\n",
    "LR12_pipeline = Pipeline([('preprocessing', full_pipeline),\n",
    "                            ('logistic', elasticnet_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters:\n",
    "\n",
    "- C: represent the inverse of regularization strength, smaller values specify stronger regularization\n",
    "- l1_ratio: is the Elastic-Net mixing parameter, l1_ratio=1 is equivalent to using penalty='l1' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters grid\n",
    "param_grid={\"logistic__C\": [0.1,0.01,1.0],\n",
    "            \"logistic__l1_ratio\":[0.25,0.5,0.75]}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "GS_elasticnet = GridSearchCV(LR12_pipeline, param_grid, cv=cv, scoring=\"f1\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Logistic regression with Elasticnet penalty\n",
      "---------------------------------\n",
      "train_score: mean = 0.612, std = 0.012\n",
      "validation_score: mean = 0.551, std = 0.026\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(GS_elasticnet, X_train, Y_train, scoring = 'f1',\n",
    "                           return_train_score = True, n_jobs=-1)\n",
    "print('---------------------------------')\n",
    "print(str('Logistic regression with Elasticnet penalty'))\n",
    "print('---------------------------------')\n",
    "error_train = scores['train_score']\n",
    "error_test = scores['test_score']\n",
    "print(f'train_score: mean = {np.round(error_train.mean(),3)}, std = {np.round(error_train.std(),3)}')\n",
    "print(f'validation_score: mean = {np.round(error_test.mean(),3)}, std = {np.round(error_test.std(),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we tried to implement a support vector machine to deal with the high-dimensionality of the data, thanks to the kernel trick. For this reason, no prior feature selection is implemented in this case. We used a Radial Basis Function kernel, which is the\n",
    "most seen in literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline: preprocessing + SVM\n",
    "SVM_pipeline = Pipeline([\n",
    "    ('preprocessing', full_pipeline),\n",
    "    ('SVM', svm.SVC(class_weight = 'balanced',random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters:\n",
    "- C: represent the inverse of the regularization strength\n",
    "- gamma: is the kernel coefficient for ‘rbf’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters grid\n",
    "param_grid = {'SVM__C': [0.5,1,2,4,6,8,10], \n",
    "              'SVM__gamma': [1, 0.1, 0.01, 0.001, 0.0001,'auto','scale']}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
    "\n",
    "RS_SVM = RandomizedSearchCV(SVM_pipeline, param_grid, cv=cv, scoring='f1',\n",
    "                         random_state=42, verbose = 1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "SVM\n",
      "------------------------------------------------------------------\n",
      "train_score: mean = 0.686, std = 0.069\n",
      "validation_score: mean = 0.527, std = 0.036\n"
     ]
    }
   ],
   "source": [
    "score = cross_validate(estimator=RS_SVM, X=X_train, y=Y_train, cv=cv,scoring=\"f1\",\n",
    "                               n_jobs = -1,return_train_score = True,error_score=\"raise\")\n",
    "\n",
    "print('------------------------------------------------------------------')\n",
    "print(str('SVM'))\n",
    "print('------------------------------------------------------------------')\n",
    "error_train = score['train_score']\n",
    "error_test = score['test_score']\n",
    "print(f'train_score: mean = {np.round(error_train.mean(),3)}, std = {np.round(error_train.std(),3)}')\n",
    "print(f'validation_score: mean = {np.round(error_test.mean(),3)}, std = {np.round(error_test.std(),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to implement a Decision Tree classifier, a non-parametric supervised learning methods which is simple to understand and to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline: preprocessing + DecisionTree\n",
    "DT_pipeline = Pipeline([\n",
    "    ('preprocessing', full_pipeline),\n",
    "    ('DT', DecisionTreeClassifier(random_state = 42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fit a simple DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "DecisionTreeClassifier(random_state=1)\n",
      "---------------------------------\n",
      "train_score: mean = 1.0, std = 0.0\n",
      "validation_score: mean = 0.419, std = 0.042\n",
      "---------------------------------\n",
      "DecisionTreeClassifier(class_weight='balanced', random_state=1)\n",
      "---------------------------------\n",
      "train_score: mean = 1.0, std = 0.0\n",
      "validation_score: mean = 0.448, std = 0.04\n",
      "---------------------------------\n",
      "DecisionTreeClassifier(max_depth=10, random_state=1)\n",
      "---------------------------------\n",
      "train_score: mean = 0.871, std = 0.029\n",
      "validation_score: mean = 0.388, std = 0.037\n",
      "---------------------------------\n",
      "DecisionTreeClassifier(class_weight='balanced', max_depth=5, random_state=1)\n",
      "---------------------------------\n",
      "train_score: mean = 0.651, std = 0.02\n",
      "validation_score: mean = 0.508, std = 0.021\n"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "clfs.append(DecisionTreeClassifier(random_state = 42))\n",
    "clfs.append(DecisionTreeClassifier(class_weight = 'balanced', random_state = 42))\n",
    "clfs.append(DecisionTreeClassifier(max_depth=10, random_state = 42))\n",
    "clfs.append(DecisionTreeClassifier(class_weight = 'balanced', max_depth=5, random_state = 42))\n",
    "\n",
    "for classifier in clfs:\n",
    "    DT_pipeline.set_params(DT = classifier)\n",
    "    scores = cross_validate(DT_pipeline, X_train, Y_train, scoring = 'f1',\n",
    "                           return_train_score = True)\n",
    "    print('---------------------------------')\n",
    "    print(str(classifier))\n",
    "    print('---------------------------------')\n",
    "    error_train = scores['train_score']\n",
    "    error_test = scores['test_score']\n",
    "    print(f'train_score: mean = {np.round(error_train.mean(),3)}, std = {np.round(error_train.std(),3)}')\n",
    "    print(f'validation_score: mean = {np.round(error_test.mean(),3)}, std = {np.round(error_test.std(),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this models we can make some considerations:\n",
    "- a plain DT strongly overfits the training set\n",
    "- using balanced weights, increases performance on test set, but still overfits training\n",
    "- setting the maximum depth of the tree decreases performance on test set, but also on training set (reduces overfitting)\n",
    "- setting both balanced weights and maximum tree depth allows for better performance on testing, while keeping a contained performance on training.\n",
    "\n",
    "With these observations in mind, we try to set some hyperparameters trough a grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Try a Grid Search to select some parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters:\n",
    "\n",
    "- criterion: it is the function to measure the quality of a split, “gini” is the supported criteria for the Gini impurity (it splits the node such that it gives the least amount of impurity) and “entropy” for the information gain (it splits the node such that it gives the most amount of information gain)\n",
    "- min_samples_split: the minimum number of samples required to split an internal node\n",
    "- max_depth: maximum depth of the tree\n",
    "\n",
    "We notice that min_samples_split is tied to max_depth and the model selects one over the other depending on which gives the maximum depth for the tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'DT__criterion': [\"gini\", \"entropy\"], \n",
    "              'DT__min_samples_split': np.linspace(0.4,1,9),\n",
    "              'DT__max_depth': np.arange(3,15)}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_pipeline.set_params(DT = DecisionTreeClassifier(class_weight='balanced', random_state = 1))\n",
    "\n",
    "DT_RS = RandomizedSearchCV(DT_pipeline, param_grid, cv=cv, scoring='f1',n_iter=50,\n",
    "                         random_state=42, verbose = 1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "DecisionTree\n",
      "------------------------------------------------------------------\n",
      "train_score: mean = 0.553, std = 0.031\n",
      "validation_score: mean = 0.524, std = 0.052\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(estimator=DT_RS, X=X_train, y=Y_train, cv=cv,scoring=\"f1\",\n",
    "                               n_jobs = -1,return_train_score = True)\n",
    "\n",
    "print('------------------------------------------------------------------')\n",
    "print(str(\"DecisionTree\"))\n",
    "print('------------------------------------------------------------------')\n",
    "error_train = scores['train_score']\n",
    "error_test = scores['test_score']\n",
    "print(f'train_score: mean = {np.round(error_train.mean(),3)}, std = {np.round(error_train.std(),3)}')\n",
    "print(f'validation_score: mean = {np.round(error_test.mean(),3)}, std = {np.round(error_test.std(),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Let's try to reduce the overfitting problem, by using ensamble methods which try to reduce the variance of the estiamtes. First, we look at the cross validation score of a vanilla RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline: preprocessing + Random Forest\n",
    "RF_pipeline = Pipeline([\n",
    "    ('preprocessing', full_pipeline),\n",
    "    ('clf', RandomForestClassifier(class_weight = 'balanced', random_state = 42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters:\n",
    "- max_depth: maximum depth of the tree\n",
    "- min_saples_leaf: minimum number of data points allowed in a leaf node\n",
    "- min_samples_split: minimum number of samples required to split an internal node\n",
    "- max_leaf_nodes: it caps the number of leaf nodes and allows the branches of a tree to have varying depths.\n",
    "- min_impurity_decrease: it is used to supervise the threshold for splitting nodes: split will take place if it reduces the Gini Impurity. Useful when there is a big quantity of features in the dataset.\n",
    "- n_estimators: number of decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters grid\n",
    "param_grid = {'clf__max_depth': np.arange(5,10), \n",
    "              'clf__min_samples_leaf': np.linspace(10,20,2).astype(int), \n",
    "              'clf__min_samples_split': np.linspace(0.4,1,3), \n",
    "              'clf__min_impurity_decrease': np.linspace(0,0.4,4), \n",
    "              'clf__max_leaf_nodes': np.linspace(10,20,2).astype(int),\n",
    "              'clf__n_estimators': [100,150]\n",
    "             }\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_RS = GridSearchCV(RF_pipeline, param_grid, cv=cv, scoring='f1',verbose = 1, n_jobs=-1)\n",
    "\n",
    "score = cross_validate(estimator=RF_RS, X=X_train, y=Y_train, cv=cv,scoring=\"f1\",\n",
    "                               n_jobs = -1,return_train_score = True,error_score=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_score: mean = 0.561, std = 0.007\n",
      "validation_score: mean = 0.505, std = 0.03\n"
     ]
    }
   ],
   "source": [
    "error_train = score['train_score']\n",
    "error_test = score['test_score']\n",
    "print(f'train_score: mean = {np.round(error_train.mean(),3)}, std = {np.round(error_train.std(),3)}')\n",
    "print(f'validation_score: mean = {np.round(error_test.mean(),3)}, std = {np.round(error_test.std(),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting some features before fitting the forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the random forest using different features selection methods:\n",
    "\n",
    "- SelectKBest is an univariate features selection method, which works by selecting the best features based on univariate statistical tests. In particular, it tests each feature against the outcome using Anova, and removing all but the k features having smallest p-value.\n",
    "\n",
    "- SelectFromModel finds the best features based on the coefficients obtained fitting the given estimator (in our case, the Elastic-net Logistic Regression which puts the weight of some features automatically to 0).\n",
    "\n",
    "- Recursive Feature Elimination uses the given estimator (Perceptron and Decision Tree in our case) to compute coefficients of the features, and recursively eliminates the one having smaller weight. The optimal number of features is selected in a CV loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline: preprocessing + feature selection method + Random Forest\n",
    "RF_pipeline = Pipeline([\n",
    "    ('preprocessing', full_pipeline),\n",
    "    ('feature_selection', SelectFromModel(elasticnet_model)),\n",
    "    ('clf', RandomForestClassifier(class_weight = 'balanced', max_depth = 5, random_state = 42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters grid\n",
    "param_grid = {'clf__max_depth': np.arange(3,10), \n",
    "              'clf__min_samples_leaf': np.linspace(1,20,5).astype(int), \n",
    "              'clf__min_samples_split': np.linspace(0.4,1,5), \n",
    "              'clf__min_impurity_decrease': np.linspace(0,0.4,4), \n",
    "              'clf__max_leaf_nodes': np.linspace(10,20,3).astype(int),\n",
    "              'clf__n_estimators': [100,150]\n",
    "             }\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "SelectKBest(k=60)\n",
      "------------------------------------------------------------------\n",
      "train_score: mean = 0.563, std = 0.008\n",
      "validation_score: mean = 0.501, std = 0.034\n",
      "------------------------------------------------------------------\n",
      "SelectFromModel(estimator=LogisticRegression(class_weight='balanced',\n",
      "                                             l1_ratio=0.5, max_iter=5000,\n",
      "                                             penalty='elasticnet',\n",
      "                                             random_state=42, solver='saga'))\n",
      "------------------------------------------------------------------\n",
      "train_score: mean = 0.566, std = 0.012\n",
      "validation_score: mean = 0.51, std = 0.029\n",
      "------------------------------------------------------------------\n",
      "RFECV(estimator=Perceptron(class_weight='balanced', random_state=42))\n",
      "------------------------------------------------------------------\n",
      "train_score: mean = 0.567, std = 0.015\n",
      "validation_score: mean = 0.508, std = 0.03\n",
      "------------------------------------------------------------------\n",
      "RFECV(estimator=DecisionTreeClassifier(class_weight='balanced', max_depth=5,\n",
      "                                       random_state=42))\n",
      "------------------------------------------------------------------\n",
      "train_score: mean = 0.555, std = 0.023\n",
      "validation_score: mean = 0.504, std = 0.042\n"
     ]
    }
   ],
   "source": [
    "#We cross-validate and compared the Random Forest classifier with different \n",
    "#features selection methods\n",
    "select = []\n",
    "select.append(SelectKBest(k=60))\n",
    "select.append(SelectFromModel(elasticnet_model))\n",
    "select.append(RFECV(estimator=Perceptron(random_state=42, class_weight='balanced')))\n",
    "select.append(RFECV(estimator=DecisionTreeClassifier(class_weight = 'balanced', max_depth=5, random_state=42)))\n",
    "\n",
    "for method in select:\n",
    "    RF_pipeline.set_params(feature_selection = method)\n",
    "    RF_RS = RandomizedSearchCV(RF_pipeline, param_grid, cv=cv, scoring='f1',n_iter=30,\n",
    "                         random_state=1, verbose = 1, n_jobs=-1)\n",
    "    \n",
    "    scores = cross_validate(estimator=RF_RS, X=X_train, y=Y_train, cv=cv,scoring=\"f1\",\n",
    "                               n_jobs = -1,return_train_score = True,error_score=\"raise\")\n",
    "\n",
    "    print('------------------------------------------------------------------')\n",
    "    print(str(method))\n",
    "    print('------------------------------------------------------------------')\n",
    "    error_train = scores['train_score']\n",
    "    error_test = scores['test_score']\n",
    "    print(f'train_score: mean = {np.round(error_train.mean(),3)}, std = {np.round(error_train.std(),3)}')\n",
    "    print(f'validation_score: mean = {np.round(error_test.mean(),3)}, std = {np.round(error_test.std(),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried also to implement an ensable method and we focused on Extreme Gradient Boost because is a more regularized form of Gradient Boosting which uses advanced regularization techniques to improve the model generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline: preprocessing + XGboost\n",
    "GB_pipeline = Pipeline([\n",
    "    ('preprocessing', full_pipeline),\n",
    "    ('clf',XGBClassifier(objective= 'binary:logistic',booster='gbtree',nthread=4,seed=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters:\n",
    "\n",
    "- gamma: minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "- colsample_bytree: subsample ratio of features when constructing each tree\n",
    "- learning_rate: step size shrinkage used in the update to prevent overfitting\n",
    "- max_depth: maximum depth of the tree\n",
    "- min_child_weight: minimum sum of instance weights needed in a child for a further partition\n",
    "- n_estimators: number of decision trees\n",
    "- scale_pos_weight: it controls the balance of positive and negative weights. Useful for unbalanced classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos = ( Y_train.shape[0]-Y_train.sum() ) / Y_train.sum()\n",
    "\n",
    "#Hyperparameters grid\n",
    "param_grid = {'clf__gamma': [1,3,5],\n",
    "              'clf__colsample_bytree': [0.6,0.65,0.75],\n",
    "              'clf__learning_rate': [0.03,0.05,0.08],\n",
    "              'clf__max_depth': [2,3,4],\n",
    "              'clf__min_child_weight': [1,5,10],\n",
    "              'clf__n_estimators': [100,150,200],\n",
    "              'clf__scale_pos_weight': [scale_pos]}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "RS_GB = RandomizedSearchCV(GB_pipeline, param_grid, cv=cv, scoring='f1',n_iter=30,\n",
    "                         random_state=1, verbose = 1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "XGBoost\n",
      "------------------------------------------------------------------\n",
      "train_score: mean = 0.728, std = 0.044\n",
      "validation_score: mean = 0.511, std = 0.034\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(estimator=RS_GB, X=X_train, y=Y_train, cv=cv,scoring=\"f1\",\n",
    "                        n_jobs = -1,return_train_score = True)\n",
    "print('------------------------------------------------------------------')\n",
    "print(str(\"XGBoost\"))\n",
    "print('------------------------------------------------------------------')\n",
    "error_train = scores['train_score']\n",
    "error_test = scores['test_score']\n",
    "print(f'train_score: mean = {np.round(error_train.mean(),3)}, std = {np.round(error_train.std(),3)}')\n",
    "print(f'validation_score: mean = {np.round(error_test.mean(),3)}, std = {np.round(error_test.std(),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticNet + SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end a trial was also done using SMOTE: this method over-samples the minority class by adding some synthetic points with similar characteristics to a minority class point.\n",
    "Since the over-sampling should only be done during the training phase, it should be added to a Pipeline able to fit the transformation over the training set but not act on the test set in the transform step. For this, we introduced a new Pipeline from the imblearn library. Then, we repeated the same process of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnet_model = LogisticRegression(penalty = 'elasticnet', solver = 'saga', \n",
    "                                 l1_ratio = 0.5, max_iter = 5000, random_state=42)\n",
    "\n",
    "#Imblearn pipeline: Smote + logistic regression\n",
    "LR12_pipeline_imb = Pipeline_imb([\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('logistic', elasticnet_model)])\n",
    "\n",
    "#Pipeline: preprocessing + Imblearn pipeline:\n",
    "LR12_smote_pipeline = Pipeline([('preprocessing', full_pipeline),\n",
    "                                ('pip_smote', LR12_pipeline_imb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Logistic regression with Elasticnet penalty\n",
      "---------------------------------\n",
      "train_score: mean = 0.609, std = 0.01\n",
      "validation_score: mean = 0.545, std = 0.031\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(LR12_smote_pipeline, X_train, Y_train, scoring = 'f1',\n",
    "                           return_train_score = True)\n",
    "print('---------------------------------')\n",
    "print(str('Logistic regression with Elasticnet penalty'))\n",
    "print('---------------------------------')\n",
    "error_train = scores['train_score']\n",
    "error_test = scores['test_score']\n",
    "print(f'train_score: mean = {np.round(error_train.mean(),3)}, std = {np.round(error_train.std(),3)}')\n",
    "print(f'validation_score: mean = {np.round(error_test.mean(),3)}, std = {np.round(error_test.std(),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure does not improve the performance of the model, rather it slightly deteriorates it. This may be due to the fact that ElasticNet class_weight = 'balanced' works already well with unbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model: Logistic Regression with ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these results, the model having the best performance in terms of $F1$ is the Logistic Regression with Elastic-Net penalty. We fit this model on all training and test set to get an estimate of its predictive capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training performances:\n",
      "AUC: 0.661, F1: 0.602\n",
      "Test performances:\n",
      "AUC: 0.607, F1: 0.539\n"
     ]
    }
   ],
   "source": [
    "GS_elasticnet.fit(X_train,Y_train)\n",
    "print('Training performances:')\n",
    "m1 = compute_metrics(Y_train, GS_elasticnet.predict(X_train))\n",
    "print('Test performances:')\n",
    "m2 = compute_metrics(Y_test, GS_elasticnet.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC on the test set is not very high but this is already a little improvement w.r.t. the results of the literature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = GS_elasticnet.best_estimator_.named_steps['logistic']\n",
    "feature_names = list(GS_elasticnet.best_estimator_.named_steps['preprocessing'][:-1].get_feature_names_out(input_features=X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features importance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have an idea of the drivers in the prediction of the readmission within six months, we show the 15 highest coefficients in the Logistic Regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importances = pow(math.e,final_model.coef_[0])\n",
    "\n",
    "importances = pd.Series(\n",
    "    feature_importances[:106], index=feature_names[:106]\n",
    ").sort_values(ascending=True)\n",
    "\n",
    "ax = importances[91:].plot.barh()\n",
    "ax.set_title(\"Logistic Regression Feature Importances\")\n",
    "ax.figure.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
